# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from __future__ import annotations

import torch
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
from torch.distributions import Normal
import os

from rsl_rl.modules import ActorCriticStreaming


class ObGD(torch.optim.Optimizer):
    """Online Batch Gradient Descentä¼˜åŒ–å™¨ï¼Œæ¥è‡ªstreaming-drlé¡¹ç›®"""
    def __init__(self, params, lr=1.0, gamma=0.99, lamda=0.8, kappa=2.0):
        defaults = dict(lr=lr, gamma=gamma, lamda=lamda, kappa=kappa)
        super(ObGD, self).__init__(params, defaults)
    
    def step(self, delta, reset=False):
        z_sum = 0.0
        for group in self.param_groups:
            for p in group["params"]:
                state = self.state[p]
                if len(state) == 0:
                    state["eligibility_trace"] = torch.zeros_like(p.data)
                e = state["eligibility_trace"]
                e.mul_(group["gamma"] * group["lamda"]).add_(p.grad, alpha=1.0)
                z_sum += e.abs().sum().item()

        delta_bar = max(abs(delta), 1.0)
        dot_product = delta_bar * z_sum * group["lr"] * group["kappa"]
        if dot_product > 1:
            step_size = group["lr"] / dot_product
        else:
            step_size = group["lr"]

        for group in self.param_groups:
            for p in group["params"]:
                state = self.state[p]
                e = state["eligibility_trace"]
                p.data.add_(delta * e, alpha=-step_size)
                if reset:
                    e.zero_()


class streamingAC:
    """Streaming ACç®—æ³•ï¼ŒåŸºäºstream_ac_continuous.pyçš„StreamACç±»å®ç°"""

    policy: ActorCriticStreaming
    """The actor critic module."""

    def __init__(
        self,
        policy,
        # StreamingACç‰¹æœ‰å‚æ•°
        lr=1.0,
        gamma=0.99,
        lamda=0.8,
        kappa_policy=3.0,
        kappa_value=2.0,
        entropy_coef=0.01,
        device="cpu",
    ):
        # è®¾å¤‡å‚æ•°
        self.device = device

        # StreamingACæ ¸å¿ƒå‚æ•°
        self.gamma = gamma
        self.lamda = lamda
        self.lr = lr
        self.kappa_policy = kappa_policy
        self.kappa_value = kappa_value
        self.entropy_coef = entropy_coef

        # ç­–ç•¥å’Œä»·å€¼ç½‘ç»œ
        self.policy = policy
        self.policy.to(self.device)
        
        # ä½¿ç”¨ObGDä¼˜åŒ–å™¨
        self.optimizer_policy = ObGD(
            [p for name, p in self.policy.named_parameters() if 'actor' in name or 'linear_mu' in name or 'linear_std' in name],
            lr=lr, gamma=gamma, lamda=lamda, kappa=kappa_policy
        )
        self.optimizer_value = ObGD(
            [p for name, p in self.policy.named_parameters() if 'critic' in name],
            lr=lr, gamma=gamma, lamda=lamda, kappa=kappa_value
        )

        print(f"StreamingAC initialized with lr={lr}, gamma={gamma}, lamda={lamda}")
        print(f"Policy kappa={kappa_policy}, Value kappa={kappa_value}, entropy_coef={entropy_coef}")

    def pi(self, x):
        """ç­–ç•¥ç½‘ç»œå‰å‘ä¼ æ’­"""
        return self.policy.actor_forward(x)

    def v(self, x):
        """ä»·å€¼ç½‘ç»œå‰å‘ä¼ æ’­"""
        return self.policy.critic_forward(x)

    def sample_action(self, obs):
        """é‡‡æ ·åŠ¨ä½œ"""
        if isinstance(obs, np.ndarray):
            obs = torch.from_numpy(obs).float().to(self.device)
        elif obs.device != self.device:
            obs = obs.to(self.device)
        
        with torch.no_grad():
            mu, std = self.pi(obs)
            dist = Normal(mu, std)
            action = dist.sample()
        
        return action

    # ğŸ”¥ğŸ”¥ğŸ”¥ é‡è¦ï¼šç½‘ç»œæ›´æ–°ç®—æ³•èµ·ç‚¹ ğŸ”¥ğŸ”¥ğŸ”¥
    def update_params(self, s, a, r, s_prime, done, overshooting_info=False):
        """æ ¸å¿ƒæ›´æ–°æ–¹æ³•ï¼ŒåŸºäºTDè¯¯å·®å’Œeligibility tracesè¿›è¡Œåœ¨çº¿å­¦ä¹ """
        done_mask = 0 if done else 1
        
        # ç¡®ä¿è¾“å…¥ä¸ºtensor
        if isinstance(s, np.ndarray):
            s = torch.tensor(s, dtype=torch.float32, device=self.device)
        if isinstance(a, np.ndarray):
            a = torch.tensor(a, dtype=torch.float32, device=self.device)
        if not isinstance(r, torch.Tensor):
            r = torch.tensor(r, dtype=torch.float32, device=self.device)
        if isinstance(s_prime, np.ndarray):
            s_prime = torch.tensor(s_prime, dtype=torch.float32, device=self.device)
        if not isinstance(done_mask, torch.Tensor):
            done_mask = torch.tensor(done_mask, dtype=torch.float32, device=self.device)

        # è®¡ç®—ä»·å€¼å‡½æ•°å’ŒTDè¯¯å·®
        v_s = self.v(s)
        v_prime = self.v(s_prime)
        td_target = r + self.gamma * v_prime * done_mask
        delta = td_target - v_s

        # è®¡ç®—ç­–ç•¥æŸå¤±
        mu, std = self.pi(s)
        dist = Normal(mu, std)
        log_prob_pi = -(dist.log_prob(a)).sum()
        
        # ä»·å€¼å‡½æ•°æŸå¤±
        value_output = -v_s
        
        # ç†µæŸå¤±ï¼ˆå¸¦ç¬¦å·ï¼‰
        entropy_pi = -self.entropy_coef * dist.entropy().sum() * torch.sign(delta).item()

        # æ¸…é›¶æ¢¯åº¦
        self.optimizer_value.zero_grad()
        self.optimizer_policy.zero_grad()
        
        # åå‘ä¼ æ’­
        # criticåå‘ä¼ æ’­ï¼Œä¿ç•™è®¡ç®—å›¾
        value_output.backward(retain_graph=True)
        # actoråå‘ä¼ æ’­ï¼Œè®¡ç®—log_prob_piå’Œentropy_piçš„æ¢¯åº¦
        (log_prob_pi + entropy_pi).backward()
        
        # ä½¿ç”¨ObGDä¼˜åŒ–å™¨æ›´æ–°å‚æ•°ï¼Œä½¿ç”¨deltaä½œä¸ºæ¢¯åº¦
        self.optimizer_policy.step(delta.item(), reset=done)
        self.optimizer_value.step(delta.item(), reset=done)

        # è¶…è°ƒæ£€æµ‹ï¼ˆå¯é€‰ï¼‰
        if overshooting_info:
            with torch.no_grad():
                v_s_new = self.v(s)
                v_prime_new = self.v(s_prime)
                td_target_new = r + self.gamma * v_prime_new * done_mask
                delta_bar = td_target_new - v_s_new
                if torch.sign(delta_bar * delta).item() == -1:
                    print("Overshooting Detected!")

        return {
            "value_function": value_output.item(),
            "policy_loss": log_prob_pi.item(),
            "entropy": dist.entropy().sum().item(),
            "td_error": delta.item()
        }
    # ğŸ”¥ğŸ”¥ğŸ”¥ é‡è¦ï¼šç½‘ç»œæ›´æ–°ç®—æ³•ç»ˆç‚¹ ğŸ”¥ğŸ”¥ğŸ”¥

    def load_pretrained_policy(self, checkpoint_path, finetune_mode="full", reset_optimizer=True):
        """
        åŠ è½½é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒ
        
        Args:
            checkpoint_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
            finetune_mode: å¾®è°ƒæ¨¡å¼
                - "full": å¾®è°ƒæ•´ä¸ªç½‘ç»œ
                - "actor_only": åªå¾®è°ƒactorç½‘ç»œï¼Œå†»ç»“critic
                - "critic_only": åªå¾®è°ƒcriticç½‘ç»œï¼Œå†»ç»“actor
                - "partial": éƒ¨åˆ†å¾®è°ƒï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è‡ªå®šä¹‰ï¼‰
            reset_optimizer: æ˜¯å¦é‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆeligibility tracesç­‰ï¼‰
        """
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {checkpoint_path}")
        
        print(f"æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # æ‰“å°checkpointçš„ç»“æ„ä¿¡æ¯
        print(f"ğŸ“‹ Checkpoint keys: {list(checkpoint.keys())}")
        
        # åŠ è½½æ¨¡å‹æƒé‡
        if "model_state_dict" in checkpoint:
            # Isaac Labæ ‡å‡†æ ¼å¼
            model_state_dict = checkpoint["model_state_dict"]
            print("ğŸ“¦ ä½¿ç”¨model_state_dictæ ¼å¼")
        elif "policy_state_dict" in checkpoint:
            # å¯èƒ½çš„å…¶ä»–æ ¼å¼
            model_state_dict = checkpoint["policy_state_dict"]
            print("ğŸ“¦ ä½¿ç”¨policy_state_dictæ ¼å¼")
        elif "ac_state_dict" in checkpoint:
            # RSL-RL PPOæ ¼å¼
            model_state_dict = checkpoint["ac_state_dict"]
            print("ğŸ“¦ ä½¿ç”¨ac_state_dictæ ¼å¼ï¼ˆPPOæ¨¡å‹ï¼‰")
        else:
            # ç›´æ¥çš„state_dict
            model_state_dict = checkpoint
            print("ğŸ“¦ ä½¿ç”¨ç›´æ¥state_dictæ ¼å¼")
        
        # æ‰“å°é¢„è®­ç»ƒæ¨¡å‹çš„ç½‘ç»œç»“æ„
        print(f"\nğŸ” é¢„è®­ç»ƒæ¨¡å‹å‚æ•°:")
        for k, v in model_state_dict.items():
            print(f"  {k}: {v.shape}")
        
        # æ‰“å°å½“å‰ç½‘ç»œç»“æ„
        current_dict = self.policy.state_dict()
        print(f"\nğŸ” å½“å‰StreamingACç½‘ç»œå‚æ•°:")
        for k, v in current_dict.items():
            print(f"  {k}: {v.shape}")
        
        # å°è¯•æ™ºèƒ½å‚æ•°æ˜ å°„
        mapped_dict = self._smart_parameter_mapping(model_state_dict, current_dict)
        
        # è¿‡æ»¤ä¸åŒ¹é…çš„é”®ï¼ˆå¦‚æœç½‘ç»œç»“æ„æœ‰å˜åŒ–ï¼‰
        policy_dict = self.policy.state_dict()
        filtered_dict = {}
        
        for k, v in mapped_dict.items():
            if k in policy_dict and v.shape == policy_dict[k].shape:
                filtered_dict[k] = v
                print(f"  âœ“ åŠ è½½å‚æ•°: {k} {v.shape}")
            else:
                if k in policy_dict:
                    print(f"  âœ— è·³è¿‡å‚æ•°: {k} (å½¢çŠ¶ä¸åŒ¹é…: é¢„è®­ç»ƒ{v.shape} vs å½“å‰{policy_dict[k].shape})")
                else:
                    print(f"  âœ— è·³è¿‡å‚æ•°: {k} (å½“å‰ç½‘ç»œä¸­ä¸å­˜åœ¨)")
        
        if len(filtered_dict) == 0:
            print("\nâš ï¸  è­¦å‘Š: æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å‚æ•°ï¼")
            print("ğŸ”§ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
            print("   1. æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹æ˜¯å¦ä¸å½“å‰ç½‘ç»œç»“æ„å…¼å®¹")
            print("   2. ç¡®è®¤é¢„è®­ç»ƒæ¨¡å‹æ ¼å¼æ˜¯å¦æ­£ç¡®")
            print("   3. è€ƒè™‘è°ƒæ•´ç½‘ç»œç»“æ„ä½¿å…¶ä¸é¢„è®­ç»ƒæ¨¡å‹åŒ¹é…")
            return False
        
        # åŠ è½½åŒ¹é…çš„å‚æ•°
        policy_dict.update(filtered_dict)
        self.policy.load_state_dict(policy_dict)
        
        print(f"\nâœ… æˆåŠŸåŠ è½½ {len(filtered_dict)}/{len(model_state_dict)} ä¸ªå‚æ•°")
        
        # æ ¹æ®å¾®è°ƒæ¨¡å¼è®¾ç½®å‚æ•°
        self._set_finetune_mode(finetune_mode)
        
        # é‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€
        if reset_optimizer:
            self.reset_optimizer_states()
            print("å·²é‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆeligibility tracesï¼‰")
        
        print(f"é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å®Œæˆï¼Œå¾®è°ƒæ¨¡å¼: {finetune_mode}")
        return True

    def _smart_parameter_mapping(self, pretrained_dict, current_dict):
        """
        æ™ºèƒ½å‚æ•°æ˜ å°„ï¼Œå°è¯•åŒ¹é…ä¸åŒå‘½åæ ¼å¼çš„å‚æ•°
        """
        mapped_dict = {}
        
        # æ™ºèƒ½æ˜ å°„ï¼ˆPPO -> StreamingACï¼‰- æ— è®ºç›´æ¥åŒ¹é…ç»“æœå¦‚ä½•éƒ½æ‰§è¡Œ
        print("ğŸ”„ å°è¯•æ™ºèƒ½æ˜ å°„...")
        ppo_to_streaming_mapping = {
            # Actor mappings: PPO(0,2,4,6) -> StreamingAC(0,1,2,3)
            'actor.0.weight': 'actor.0.weight',
            'actor.0.bias': 'actor.0.bias', 
            'actor.2.weight': 'actor.1.weight',  # PPOç¬¬2å±‚ -> StreamingACç¬¬1å±‚
            'actor.2.bias': 'actor.1.bias',
            'actor.4.weight': 'actor.2.weight',  # PPOç¬¬4å±‚ -> StreamingACç¬¬2å±‚
            'actor.4.bias': 'actor.2.bias',
            'actor.6.weight': 'actor.3.weight',  # PPOç¬¬6å±‚ -> StreamingACç¬¬3å±‚ï¼ˆè¾“å‡ºå±‚ï¼‰
            'actor.6.bias': 'actor.3.bias',
            'std': 'std',  # PPOçš„stdå‚æ•°ç›´æ¥æ˜ å°„
            
            # Critic mappings: PPO(0,2,4,6) -> StreamingAC(0,1,2,3)
            'critic.0.weight': 'critic.0.weight',
            'critic.0.bias': 'critic.0.bias',
            'critic.2.weight': 'critic.1.weight',  # PPOç¬¬2å±‚ -> StreamingACç¬¬1å±‚
            'critic.2.bias': 'critic.1.bias',
            'critic.4.weight': 'critic.2.weight',  # PPOç¬¬4å±‚ -> StreamingACç¬¬2å±‚
            'critic.4.bias': 'critic.2.bias',
            'critic.6.weight': 'critic.3.weight',  # PPOç¬¬6å±‚ -> StreamingACç¬¬3å±‚ï¼ˆè¾“å‡ºå±‚ï¼‰
            'critic.6.bias': 'critic.3.bias'
        }
        
        for pretrained_key, current_key in ppo_to_streaming_mapping.items():
            if pretrained_key in pretrained_dict and current_key in current_dict:
                if current_key not in mapped_dict:  # é¿å…é‡å¤æ˜ å°„
                    mapped_dict[current_key] = pretrained_dict[pretrained_key]
                    print(f"  ğŸ“Œ æ™ºèƒ½æ˜ å°„: {pretrained_key} -> {current_key}")
        
        
        print(f"ğŸ“Š æ˜ å°„ç»“æœ: {len(mapped_dict)}/{len(current_dict)} ä¸ªå‚æ•°æˆåŠŸæ˜ å°„")
        return mapped_dict

    def _set_finetune_mode(self, mode):
        """è®¾ç½®å¾®è°ƒæ¨¡å¼ï¼Œå†³å®šå“ªäº›å‚æ•°å¯ä»¥è®­ç»ƒ"""
        if mode == "full":
            # å¾®è°ƒæ•´ä¸ªç½‘ç»œ
            for param in self.policy.parameters():
                param.requires_grad = True
        elif mode == "actor_only":
            # åªå¾®è°ƒactorï¼Œå†»ç»“critic
            for name, param in self.policy.named_parameters():
                if 'actor' in name or 'linear_mu' in name or 'linear_std' in name:
                    param.requires_grad = True
                elif 'critic' in name:
                    param.requires_grad = False
        elif mode == "critic_only":
            # åªå¾®è°ƒcriticï¼Œå†»ç»“actor
            for name, param in self.policy.named_parameters():
                if 'critic' in name:
                    param.requires_grad = True
                elif 'actor' in name or 'linear_mu' in name or 'linear_std' in name:
                    param.requires_grad = False
        elif mode == "partial":
            # ç¤ºä¾‹ï¼šåªå¾®è°ƒæœ€åå‡ å±‚
            for name, param in self.policy.named_parameters():
                # å¯ä»¥æ ¹æ®éœ€è¦è‡ªå®šä¹‰å“ªäº›å±‚å‚ä¸å¾®è°ƒ
                if 'output' in name or 'final' in name:
                    param.requires_grad = True
                else:
                    param.requires_grad = False
        
        # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼ˆåªåŒ…å«éœ€è¦è®­ç»ƒçš„å‚æ•°ï¼‰
        self.optimizer_policy = ObGD(
            [p for name, p in self.policy.named_parameters() 
             if ('actor' in name or 'linear_mu' in name or 'linear_std' in name) and p.requires_grad],
            lr=self.lr, gamma=self.gamma, lamda=self.lamda, kappa=self.kappa_policy
        )
        self.optimizer_value = ObGD(
            [p for name, p in self.policy.named_parameters() 
             if 'critic' in name and p.requires_grad],
            lr=self.lr, gamma=self.gamma, lamda=self.lamda, kappa=self.kappa_value
        )

    def reset_optimizer_states(self):
        """é‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œæ¸…ç©ºeligibility traces"""
        for optimizer in [self.optimizer_policy, self.optimizer_value]:
            for group in optimizer.param_groups:
                for p in group["params"]:
                    if p in optimizer.state:
                        optimizer.state[p]["eligibility_trace"] = torch.zeros_like(p.data)
        print("å·²é‡ç½®æ‰€æœ‰eligibility traces")

    def save_checkpoint(self, save_path, extra_info=None):
        """ä¿å­˜å½“å‰æ¨¡å‹çŠ¶æ€"""
        checkpoint = {
            "model_state_dict": self.policy.state_dict(),
            "optimizer_policy_state": self.optimizer_policy.state_dict(),
            "optimizer_value_state": self.optimizer_value.state_dict(),
            "algorithm_params": {
                "lr": self.lr,
                "gamma": self.gamma,
                "lamda": self.lamda,
                "kappa_policy": self.kappa_policy,
                "kappa_value": self.kappa_value,
                "entropy_coef": self.entropy_coef
            }
        }
        
        if extra_info:
            checkpoint.update(extra_info)
        
        torch.save(checkpoint, save_path)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

    def adjust_learning_rate(self, new_lr):
        """è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå¾®è°ƒæ—¶å¯èƒ½éœ€è¦ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡ï¼‰"""
        self.lr = new_lr
        for optimizer in [self.optimizer_policy, self.optimizer_value]:
            for group in optimizer.param_groups:
                group["lr"] = new_lr
        print(f"å­¦ä¹ ç‡å·²è°ƒæ•´ä¸º: {new_lr}")
